{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":64148,"databundleVersionId":7669720,"sourceType":"competition"},{"sourceId":7923451,"sourceType":"datasetVersion","datasetId":4633221},{"sourceId":28785,"sourceType":"modelInstanceVersion","modelInstanceId":8318}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\n\n## Gemma for Beginners with Hugging Face\nIn this notebook, we'll learn the very basics of using the Gemma model, incorporating the powerful tools from Hugging Face. It's focused on the simplest content without any complex processing. This practical exercise is about training a Large Language Model (LLM) to generate ***Basic Concepts of Data Science*** using the Gemma model with the support of Hugging Face libraries.\n\n## Dataset Used\n* [1000-Data-Science-Concepts](https://www.kaggle.com/datasets/hserdaraltan/1000-data-science-concepts) : This dataset covers more than 1000 common data science concepts. It covers several topics related to Statistics, Machine Learning, and Artificial Intelligence. It has two columns, one of which is questions or instructions, the other is responses to these instructions.","metadata":{}},{"cell_type":"markdown","source":"## 1. What is Gemma?\n\n**Gemma Model** is a collection of lightweight open-source generative AI (GenAI) models developed by Google DeepMind. These models are primarily aimed at developers and researchers. Gemma was released alongside Gemini, Google's closed-source generative AI chatbots.\n\nThere are two main models in the Gemma collection: **Gemma 2B** and **Gemma 7B**. These models are text-to-text decoder large language models (LLMs) with pretrained and instruction-tuned variants. Gemma 2B has a neural network with 2 billion parameters, while Gemma 7B has a neural network with seven billion parameters.\n\nGoogle offers pretrained and instruction-tuned Gemma models suitable for running on laptops and workstations. These models are available to developers through various platforms. Additionally, Meta's Llama 2 is another open-source AI model designed to run on laptops, serving as more of a business tool compared to Gemma. Gemma is often favored for scientific tasks, while Llama 2 is considered more suitable for general-purpose tasks.\n\n### Inputs and Outputs\n* Input: Gemma models take in text strings, which can range from questions and prompts to longer documents that require summarization.\n* Output: In response, they generate text in English, offering answers, summaries, or other forms of text-based output, tailored to the input provided.\n\n<div style=\"text-align:center;\">\n    <img src=\"https://www.kaggle.com/competitions/64148/images/header\" alt=\"Gemma Model\" style=\"width:50%;\"/>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"## 2. Package Installation and Importing Libraries\n\n* `huggingface_hub`: This library provides access to models, datasets, and other resources shared by the Hugging Face community.\n\n* `transformers`: Formerly known as `pytorch-transformers` or `pytorch-pretrained-bert`, this library is developed by Hugging Face. It provides state-of-the-art pre-trained models for natural language understanding (NLU) and natural language generation (NLG) tasks.\n\n* `accelerate`: Accelerate is a library developed by Hugging Face that simplifies distributed training for deep learning models and provides an easy-to-use interface for distributed computing frameworks.\n\n* `BitsAndBytes`: This library provides functions and utilities for working with binary data in Python. It includes functions for performing bitwise operations.\n\n* `trl`: The Text Representation Learning (TRL) library is developed by Hugging Face and provides tools and utilities for training and fine-tuning text representations.\n\n* `peft`: PEFT (PyTorch Extensible Fine-Tuning) is a library that extends PyTorch for fine-tuning large language models (LLMs) such as GPT and BERT.","metadata":{}},{"cell_type":"code","source":"!pip install -q -U huggingface_hub\n!pip install -q -U transformers\n!pip install -q -U accelerate\n!pip install -q -U BitsAndBytes\n%pip install -q trl\n%pip install -q peft","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-28T20:44:01.328620Z","iopub.execute_input":"2024-04-28T20:44:01.329616Z","iopub.status.idle":"2024-04-28T20:45:33.963582Z","shell.execute_reply.started":"2024-04-28T20:44:01.329581Z","shell.execute_reply":"2024-04-28T20:45:33.962486Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Python basic module**\n* `os`: Provides ways to interact with the operating system and its environment variables.\n* `torch`: PyTorch library for deep learning applications.\n* `pandas`: Powerful data processing tool, ideal for handling CSV files and other forms of structured data.\n* `re` : Provides support for working with regular expressions, enabling powerful pattern-based string operations.\n\n**Transformers module**\n* `AutoTokenizer`: Used to automatically load a pre-trained tokenizer.\n* `AutoModelForCausalLM`: Used to automatically load pre-trained models for causal language modeling.\n* `BitsAndBytesConfig`: Configuration class for setting up the Bits and Bytes tokenizer.\n* `AutoConfig`: Used to automatically load the model's configuration.\n* `TrainingArguments`: Defines arguments for training setup.\n\n**Wordcloud module**\n* `WordCloud` : Python library used for generating word clouds, which are visual representations of text data where the size of each word indicates its frequency or importance.\n* `STOPWORDS` : set of commonly used words that are often excluded from text analysis because they typically do not carry significant meaning or contribute to the understanding of the text. \n\n**Datasets module**\n* `Dataset`: A class for handling datasets.\n\n**Peft module**\n* `LoraConfig` : A configuration class for configuring the Lora model.\n* `PeftModel`: A class that defines the PEFT model.\n* `prepare_model_for_kbit_training` : A function that prepares a model for k-bit training.\n* `get_peft_model` : Function to get the PEFT model.\n\n**trl module**\n* `SFTTrainer`: Trainer class for SFT (Supervised Fine-Tuning) training.\n\n**IPython.display module**\n* `Markdown` : Used to output text in Markdown format.\n* `display` : Used to display objects in Jupyter notebooks.","metadata":{}},{"cell_type":"code","source":"import torch\nimport os\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nfrom transformers import AutoTokenizer, AutoModelForCausalLM,BitsAndBytesConfig, AutoConfig, TrainingArguments, pipeline\nfrom wordcloud import WordCloud, STOPWORDS\nfrom datasets import Dataset\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nfrom trl import SFTTrainer\nfrom IPython.display import Markdown as md\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-28T20:45:54.735822Z","iopub.execute_input":"2024-04-28T20:45:54.736469Z","iopub.status.idle":"2024-04-28T20:46:13.509416Z","shell.execute_reply.started":"2024-04-28T20:45:54.736428Z","shell.execute_reply":"2024-04-28T20:46:13.508615Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-04-28 20:46:03.836628: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-28 20:46:03.836729: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-28 20:46:03.956971: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\naccess_token=UserSecretsClient().get_secret('HUGGING_FACE')\nlogin(token=access_token)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-28T20:46:17.169505Z","iopub.execute_input":"2024-04-28T20:46:17.170546Z","iopub.status.idle":"2024-04-28T20:46:18.197547Z","shell.execute_reply.started":"2024-04-28T20:46:17.170511Z","shell.execute_reply":"2024-04-28T20:46:18.196196Z"},"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mBackendError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m login\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkaggle_secrets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UserSecretsClient\n\u001b[0;32m----> 4\u001b[0m access_token\u001b[38;5;241m=\u001b[39m\u001b[43mUserSecretsClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_secret\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHUGGING_FACE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m login(token\u001b[38;5;241m=\u001b[39maccess_token)\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/kaggle_secrets.py:64\u001b[0m, in \u001b[0;36mUserSecretsClient.get_secret\u001b[0;34m(self, label)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ValidationError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel must be non-empty.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m request_body \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m'\u001b[39m: label,\n\u001b[1;32m     63\u001b[0m }\n\u001b[0;32m---> 64\u001b[0m response_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweb_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_post_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET_USER_SECRET_BY_LABEL_ENDPOINT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecret\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m response_json:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BackendError(\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnexpected response from the service. Response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse_json\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/kaggle_web_client.py:49\u001b[0m, in \u001b[0;36mKaggleWebClient.make_post_request\u001b[0;34m(self, data, endpoint, timeout)\u001b[0m\n\u001b[1;32m     47\u001b[0m         response_json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response_json\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwasSuccessful\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m response_json:\n\u001b[0;32m---> 49\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m BackendError(\n\u001b[1;32m     50\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnexpected response from the service. Response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse_json\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response_json[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (URLError, socket\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;28;01mas\u001b[39;00m e:\n","\u001b[0;31mBackendError\u001b[0m: Unexpected response from the service. Response: {'errors': ['No user secrets exist for kernel id 55587985 and label HUGGING_FACE.'], 'error': {'code': 5, 'details': []}, 'wasSuccessful': False}."],"ename":"BackendError","evalue":"Unexpected response from the service. Response: {'errors': ['No user secrets exist for kernel id 55587985 and label HUGGING_FACE.'], 'error': {'code': 5, 'details': []}, 'wasSuccessful': False}.","output_type":"error"}]},{"cell_type":"markdown","source":"#### Check if CUDA is available","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-28T20:46:23.623753Z","iopub.execute_input":"2024-04-28T20:46:23.624418Z","iopub.status.idle":"2024-04-28T20:46:23.629485Z","shell.execute_reply.started":"2024-04-28T20:46:23.624384Z","shell.execute_reply":"2024-04-28T20:46:23.628586Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 3. Loading Gemma Model\n\n* `BitsAndBytesConfig` : The quantization_config object defines the configuration for quantization using the BitsAndBytes library. Here are the key arguments:\n\n    * **load_in_4bit (bool, optional)**: Enables 4-bit quantization, reducing memory usage by approximately fourfold compared to the original model.\n    * **bnb_4bit_quant_type (str, optional)**: Specifies the type of 4-bit quantization to use. Here, it's set to \"nf4\", a specific quantization format supported by BitsAndBytes.\n    * **bnb_4bit_compute_dtype (torch.dtype, optional)**: Defines the data type used for computations during inference. Here, it's set to torch.bfloat16, a lower-precision format that can improve speed on compatible hardware.\n\n\n* `Loading Tokenizer and Model with Quantization` :\n\n    * **AutoTokenizer**: The AutoTokenizer.from_pretrained function loads the tokenizer associated with the pre-trained model at the specified path (model). The quantization_config argument is crucial here. It tells the tokenizer to consider the quantization information (e.g., potential padding changes) while processing text.\n\n    * **AutoModelForCausalLM**: Similarly, AutoModelForCausalLM.from_pretrained loads the actual LLM model from the path (model). Again, the device_map=\"auto\" argument allows automatic device placement (CPU or GPU), and the quantization_config ensures the model is loaded with the 4-bit quantization configuration.\n\nOverall, this code snippet aims to achieve two goals:\n\n* **Load a pre-trained LLM**: It retrieves a pre-trained causal language model from the specified path.\n* **Enable Quantization for Efficiency**: By using the BitsAndBytesConfig and arguments during loading, the code configures the tokenizer and model to leverage 4-bit quantization for memory reduction and potentially faster inference on compatible hardware.","metadata":{}},{"cell_type":"code","source":"%%time\ntokenizer= AutoTokenizer.from_pretrained(\"/kaggle/input/gemma/transformers/2b-it/3\")\nquantization_config=BitsAndBytesConfig(\n                    load_in_4bit=True,\n                    bnb_4bit_use_double_quant=True,\n                    bnb_4bit_quant_type='nf4',\n                    bnb_4bit_compute_dtype=torch.bfloat16,)\nmodel = AutoModelForCausalLM.from_pretrained(\"/kaggle/input/gemma/transformers/2b-it/3\",quantization_config=quantization_config,low_cpu_mem_usage=True)\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T20:46:30.164984Z","iopub.execute_input":"2024-04-28T20:46:30.165604Z","iopub.status.idle":"2024-04-28T20:47:11.418616Z","shell.execute_reply.started":"2024-04-28T20:46:30.165568Z","shell.execute_reply":"2024-04-28T20:47:11.417615Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Gemma's activation function should be approximate GeLU and not exact GeLU.\nChanging the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0282fa79800493f872bd0a7bc6a62ba"}},"metadata":{}},{"name":"stdout","text":"GemmaForCausalLM(\n  (model): GemmaModel(\n    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n    (layers): ModuleList(\n      (0-17): 18 x GemmaDecoderLayer(\n        (self_attn): GemmaSdpaAttention(\n          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n          (up_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n          (down_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n          (act_fn): PytorchGELUTanh()\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n    )\n    (norm): GemmaRMSNorm()\n  )\n  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n)\nCPU times: user 7.85 s, sys: 4.88 s, total: 12.7 s\nWall time: 41.2 s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 4. Q & A Using Gemma\n\n* The code measures the execution time of generating a text summary using a pre-trained Gemma model. It initializes an input text, tokenizes it, and generates a summary using the model. \n* The generated summary is then decoded and printed. \n* This process is timed using the `%%time` magic command.The execution time of the entire process is displayed. \n* The Gemma model utilizes the GPU for faster computation. \n* The summary length is limited to 256 tokens.","metadata":{}},{"cell_type":"code","source":"#---------------------------------------------------------------------------\n\nimport time\nimport torch\n\ninput_text = 'Answer common questions about the Python programming language.'\ninput_ids = tokenizer(input_text, return_tensors='pt').to(\"cuda\")\n\n# Function to get memory usage on CUDA device\ndef get_cuda_memory_usage():\n    return torch.cuda.memory_allocated() / 1024 / 1024  # Memory usage in MB\n\n# Memory usage before execution\nmemory_before = get_cuda_memory_usage()\n\n# Execution\noutputs = model.generate(**input_ids, max_new_tokens=256)\n\n# Memory usage after execution\nmemory_after = get_cuda_memory_usage()\n\nmemory_used = memory_after - memory_before\n\nprint(tokenizer.decode(outputs[0]))\nprint('')\nprint(\"Memory used:\", memory_used, \"MB\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T20:48:14.494936Z","iopub.execute_input":"2024-04-28T20:48:14.495616Z","iopub.status.idle":"2024-04-28T20:48:28.184602Z","shell.execute_reply.started":"2024-04-28T20:48:14.495582Z","shell.execute_reply":"2024-04-28T20:48:28.183673Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"<bos>Answer common questions about the Python programming language.\n\n**1. What is Python?**\n\n* Python is a high-level, interpreted programming language.\n* It is known for its clear and concise syntax, making it easier to learn and use than other programming languages.\n* Python is widely used for various purposes, including data science, machine learning, web development, and scripting.\n\n**2. What are the key features of Python?**\n\n* **Dynamic typing:** Python does not require you to explicitly declare the data type of variables.\n* **Indentation:** Python uses indentation to define blocks of code, making it clear and readable.\n* **Modules:** Python has a vast collection of modules that extend the functionality of the language.\n* **Concurrency:** Python supports multithreading, allowing multiple tasks to run concurrently.\n* **Regular expressions:** Python provides powerful regular expression capabilities for text manipulation.\n\n**3. What are the different types of data in Python?**\n\n* **Numbers:** Python supports various numerical data types, including integers and floating-point numbers.\n* **Strings:** Strings are sequences of characters enclosed in double quotes.\n* **Booleans:** True and False are used to represent boolean values.\n* **Lists:** Lists are ordered collections of items of the same\n\nMemory used: 8.13623046875 MB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 5. Load Dataset\n\nLoading your data is the first step in the machine learning pipeline. This section will guide you through loading your dataset into the Jupyter notebook environment.\n\nTo download a dataset, follow these simple steps:\n1. Look for the \"Input\" option located below the \"Notebook\" section in the right-side menu.\n2. Click on the \"+ Add Input\" button.\n3. In the search bar that appears, type \"1000+-data-science-concepts\".\n4. Find the dataset in the search results and click the \"+\" button to add it to your notebook. This action will automatically download the dataset for you.","metadata":{}},{"cell_type":"code","source":"data=pd.read_csv('/kaggle/input/1000-data-science-concepts/data_science_concepts.csv')\ndataset= Dataset.from_pandas(data)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T21:21:27.806669Z","iopub.execute_input":"2024-04-13T21:21:27.807039Z","iopub.status.idle":"2024-04-13T21:21:27.832481Z","shell.execute_reply.started":"2024-04-13T21:21:27.807004Z","shell.execute_reply":"2024-04-13T21:21:27.831767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Dataset Information and Null Value Check\n* Check the number of features and values in dataset.\n* Check for any NULL values in the dataset","metadata":{}},{"cell_type":"code","source":"print(\"Information of Dataset: \")\nprint(data.info(),'\\n')\n\nprint(\"Check for NULL values: \")\nprint(data.isnull().sum().sum())","metadata":{"execution":{"iopub.status.busy":"2024-04-13T21:21:27.833756Z","iopub.execute_input":"2024-04-13T21:21:27.834415Z","iopub.status.idle":"2024-04-13T21:21:27.845146Z","shell.execute_reply.started":"2024-04-13T21:21:27.83438Z","shell.execute_reply":"2024-04-13T21:21:27.844139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Visualize Data\nThis code generates word clouds for each column in a DataFrame (`data`). Here's a step-by-step explanation:\n\n1. **Initialization**: Initialize an empty string `comment_words` to store concatenated words from all columns of the dataset and define a set of stopwords using the `STOPWORDS` set from the `wordcloud` library.\n\n2. **Subplots Creation**: Create subplots with a single row and a number of columns equal to the number of columns in the dataset. Set the figure size to (10, 6) to control the overall size of the plot.\n\n3. **Colormap Definition**: Define the colormap to be used for generating the WordCloud images. In this case, the 'viridis' colormap is chosen.\n\n4. **Iteration through Columns**: Iterate through each column in the dataset, extracting the column name and its corresponding values. Concatenate all values in the column into a single string, converting them to uppercase.\n\n5. **WordCloud Generation**: Generate a WordCloud for each column using the concatenated string of values. Customize the WordCloud's width, height, stopwords, minimum font size, and colormap. Plot each WordCloud image on the respective subplot, setting the title of each subplot to indicate the column it represents. Finally, adjust the layout and display the plot.\n\nOverall, this code visualizes the distribution of words in each column of the DataFrame by creating word clouds, providing insights into the most frequent words or terms within each column.","metadata":{}},{"cell_type":"code","source":"comment_words = ''\nstopwords = set(STOPWORDS)\n\n# Create subplots\nfig, axs = plt.subplots(1, len(data.columns), figsize=(10, 6))\n\n# Define the colormap\ncolormap = 'viridis'\n\n# Iterate through the csv file\nfor i, col in enumerate(data.columns):\n    # Concatenate all values in the column into a single string\n    # and convert to lowercase\n    comment_words += ' '.join(str(val).upper() for val in data[col]) + ' '\n\n    # Generate WordCloud for the current column\n    wordcloud = WordCloud(width=500, height=500,\n                          stopwords=stopwords,\n                          min_font_size=8,\n                          colormap=colormap).generate(comment_words)\n\n    # Plot the WordCloud image\n    axs[i].imshow(wordcloud, interpolation='bilinear')\n    axs[i].axis(\"off\")\n    axs[i].set_title(f\"Word Cloud for {col}\")\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T21:21:27.846303Z","iopub.execute_input":"2024-04-13T21:21:27.846581Z","iopub.status.idle":"2024-04-13T21:21:29.523204Z","shell.execute_reply.started":"2024-04-13T21:21:27.846558Z","shell.execute_reply":"2024-04-13T21:21:29.522326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Defining Functions\nThis Python code defines two functions:\n\n1. **generate_DS_answers** : This function generates answers to a given prompt related to Data Science. It first constructs a prompt template using a provided context. Then, it creates a message containing the prompt using the tokenizer's apply_chat_template method. Next, it encodes the prompt into tokens, sends it to the GPU for processing, generates a response using the model, and decodes the output tokens into text. Finally, it returns the generated response.\n\n2. **extract_content** : This function extracts the content from a given text that comes after the marker <start_of_turn>model. It searches for this marker in the text and returns the content that follows it. If the marker is not found, it returns a message indicating that the content was not found.\n\nThese functions work together to generate responses to prompts related to Data Science and extract the relevant content from generated text.","metadata":{}},{"cell_type":"code","source":"max_new_tokens=300\ndef generate_DS_answers(context):\n    prompt_template=f\"\"\"Provide the Answer for the following Question in 300 words.\n    Provide only useful information:\n    Context:{context}\n    \n    Output: \"\"\"\n    \n    messages=[\n        {\"role\": \"user\",\"content\": prompt_template},\n        ]\n    prompt= tokenizer.apply_chat_template(messages,tokenize=False,add_generation_prompt=True)\n    input_ids=tokenizer.encode(prompt,add_special_tokens=True,return_tensors='pt').to(\"cuda\")\n    \n    # 100 tokens = 75 words\n    outputs=model.generate(input_ids,max_new_tokens=500)\n\n    response=tokenizer.decode(outputs[0])\n    \n    return response\n\ndef extract_content(text):\n    index=text.find('<start_of_turn>model')\n  \n    if index!=-1:\n        content_after_model=text[index+len('<start_of_turn>model'):].strip()\n    else:\n        return \"Content not found after '<start_of_turn>model'\"\n    return content_after_model\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T21:22:26.030626Z","iopub.execute_input":"2024-04-13T21:22:26.031375Z","iopub.status.idle":"2024-04-13T21:22:26.039326Z","shell.execute_reply.started":"2024-04-13T21:22:26.031339Z","shell.execute_reply":"2024-04-13T21:22:26.038329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context=\"Question: \"+dataset['Question'][0]\nprint(context)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T21:22:26.04106Z","iopub.execute_input":"2024-04-13T21:22:26.041388Z","iopub.status.idle":"2024-04-13T21:22:26.051741Z","shell.execute_reply.started":"2024-04-13T21:22:26.041363Z","shell.execute_reply":"2024-04-13T21:22:26.050865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Generate Response for a given input from Dataset","metadata":{}},{"cell_type":"code","source":"Answers=generate_DS_answers(context)\nprint(Answers)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T21:22:26.053755Z","iopub.execute_input":"2024-04-13T21:22:26.054169Z","iopub.status.idle":"2024-04-13T21:22:37.873899Z","shell.execute_reply.started":"2024-04-13T21:22:26.054112Z","shell.execute_reply":"2024-04-13T21:22:37.872856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Check for Relevant Information","metadata":{}},{"cell_type":"code","source":"Answers=extract_content(Answers)\nmd(Answers)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T21:22:37.876035Z","iopub.execute_input":"2024-04-13T21:22:37.876438Z","iopub.status.idle":"2024-04-13T21:22:37.883356Z","shell.execute_reply.started":"2024-04-13T21:22:37.876402Z","shell.execute_reply":"2024-04-13T21:22:37.882439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8. Test Model before Fine Tuning\n\nBefore we start the finetuning process, let's see how the Gemma model performs out of the box on our dataset. This section will show you how to run a simple question-answering test.","metadata":{}},{"cell_type":"code","source":"for i in range(3):\n    context = \"### Question: \" + dataset['Question'][i]\n    display(md(context))\n    display(md(\"### Answer: \"))\n    display(md(extract_content(generate_DS_answers(context))))\n    print('\\n\\n')","metadata":{"execution":{"iopub.status.busy":"2024-04-13T21:22:37.884347Z","iopub.execute_input":"2024-04-13T21:22:37.884602Z","iopub.status.idle":"2024-04-13T21:23:11.626164Z","shell.execute_reply.started":"2024-04-13T21:22:37.884579Z","shell.execute_reply":"2024-04-13T21:23:11.625221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 9. Fine Tune Gemma\n\n* Define a formatting function for the model output.\n\n* WANDB is a useful tool for experiment tracking in machine learning. You might disable WANDB if you don't need experiment tracking or for debugging purposes.\n","metadata":{}},{"cell_type":"code","source":"def formatting_func(example):\n    template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n    line = template.format(instruction=example['Question'], response=example['Answer'])\n    return [line]\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2024-04-13T21:23:11.628884Z","iopub.execute_input":"2024-04-13T21:23:11.629335Z","iopub.status.idle":"2024-04-13T21:23:11.635286Z","shell.execute_reply.started":"2024-04-13T21:23:11.629298Z","shell.execute_reply":"2024-04-13T21:23:11.634287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LoRA - Low-Rank Adaptation\n\n**LoRA** is a technique used to fine-tune large language models (LLMs) more efficiently. It allows you to adapt pre-trained models to new tasks with minimal memory and computational cost compared to traditional fine-tuning.\n\nLoraConfig Parameters:\n\n* `r (int)`: This parameter defines the rank of the low-rank decomposition used in LoRA. A lower value of r uses less memory but might lead to slightly lower accuracy. The default value is typically 8, as set in our code.\n\n* `target_modules (List[str])`: This list specifies the Transformer layers where LoRA will be applied.\n    * q_proj: Query projection\n    * o_proj: Output projection\n    * k_proj: Key projection\n    * v_proj: Value projection\n    * gate_proj: Gate projection (used in attention layers)\n    * up_proj: Upsampling projection (used in some encoder-decoder architectures)\n    * down_proj: Downsampling projection (used in some encoder-decoder architectures)\n    \nBy applying LoRA to these projection layers, the model can learn task-specific adaptations without modifying the original large model weights significantly.\n\n* `task_type (str, optional)`: This parameter specifies the type of task you're fine-tuning the model for. While not used in this specific configuration, some libraries might leverage this information to optimize LoRA for specific task categories (e.g., \"CAUSAL_LM\" for causal language modeling).","metadata":{}},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r = 8,\n    target_modules = [\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n    task_type = \"CAUSAL_LM\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T21:23:11.636483Z","iopub.execute_input":"2024-04-13T21:23:11.636831Z","iopub.status.idle":"2024-04-13T21:23:11.646046Z","shell.execute_reply.started":"2024-04-13T21:23:11.636778Z","shell.execute_reply":"2024-04-13T21:23:11.64517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SFTTrainer for Supervised Fine-Tuning\n\nThis code snippet configures and initializes an SFTTrainer for fine-tuning a pre-trained model with LoRA for memory efficiency. The training hyperparameters are set within the TrainingArguments object.\n\nSFTTrainer is used to fine-tune a pre-trained model (model) on a specific training dataset (dataset). It's designed for tasks where you have labeled data and want to adapt the model for a new purpose.\n\n**Key Parameters** :\n\n* `model (PreTrainedModel)`: This argument specifies the pre-trained model you want to fine-tune.\n\n* `train_dataset (Dataset)`: This argument points to the training dataset you'll use for fine-tuning. The dataset should be formatted appropriately for the task.\n\n* `max_seq_length (int)`: This parameter defines the maximum sequence length allowed in the training data.\n\n* `args (TrainingArguments)`: This argument is an instance of TrainingArguments that defines various hyperparameters for the training process. Here are some notable arguments within args:\n\n    * **per_device_train_batch_size (int)** : Sets the batch size per device (GPU/TPU) during training.\n    * **gradient_accumulation_steps (int)** : This parameter allows accumulating gradients over several batches before updating the model weights.\n    * **warmup_steps (int)** : This defines the number of warmup steps where the learning rate is gradually increased from 0 to its full value.\n    * **max_steps (int)** : This parameter specifies the total number of training steps.\n    * **learning_rate (float)** : This sets the learning rate for the optimizer. Here, it's set to 2e-4, which is a common starting point for fine-tuning.\n    * **fp16 (bool)** : Enables training using 16-bit floating-point precision (mixed precision) for faster training with minimal accuracy loss (if supported by your hardware).\n    * **logging_steps (int)** : Defines how often training metrics are logged during training.\n    * **output_dir (str)** : Specifies the directory where training outputs (model checkpoints, logs, etc.) will be saved.\n    * **optim (str)** : Defines the optimizer used for training. Here, it's set to \"paged_adamw_8bit\", which is likely an optimizer with specific memory optimizations.\n\n* `peft_config (LoraConfig)`:It provides the configuration for LoRA (Low-Rank Adaptation), which helps fine-tune the model more efficiently.\n\n* `formatting_func (Callable)`: This argument specifies a custom function for formatting the training data before feeding it to the model. This allows for specific pre-processing steps tailored to your task.","metadata":{}},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    max_seq_length=512,\n    args=TrainingArguments(\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=4,\n        warmup_steps=2,\n        max_steps=100,\n        learning_rate=2e-4,\n        fp16=True,\n        logging_steps=1,\n        output_dir=\"outputs\",\n        optim=\"paged_adamw_8bit\"\n    ),\n    peft_config=lora_config,\n    formatting_func=formatting_func,\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T21:23:11.647072Z","iopub.execute_input":"2024-04-13T21:23:11.647336Z","iopub.status.idle":"2024-04-13T21:23:13.454591Z","shell.execute_reply.started":"2024-04-13T21:23:11.647312Z","shell.execute_reply":"2024-04-13T21:23:13.453822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T21:23:13.455844Z","iopub.execute_input":"2024-04-13T21:23:13.456619Z","iopub.status.idle":"2024-04-13T21:27:11.155986Z","shell.execute_reply.started":"2024-04-13T21:23:13.456577Z","shell.execute_reply":"2024-04-13T21:27:11.155032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 10. Test Fine- Tune Model\n\nAfter training, let's see how much our Gemma model has improved. We'll rerun the question-answering test and compare the results to the pre-finetuning performance.","metadata":{}},{"cell_type":"code","source":"context = \"### Question: \" + \"What are the Basic Concepts of Data Science?\"\ndisplay(md(context))\ndisplay(md(\"### Answer: \"))\ndisplay(md(extract_content(generate_DS_answers(context))))\nprint('\\n')","metadata":{"execution":{"iopub.status.busy":"2024-04-13T21:27:11.157171Z","iopub.execute_input":"2024-04-13T21:27:11.157447Z","iopub.status.idle":"2024-04-13T21:27:40.489553Z","shell.execute_reply.started":"2024-04-13T21:27:11.157423Z","shell.execute_reply":"2024-04-13T21:27:40.488587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(3):\n    context = \"### Question: \" + dataset['Question'][i]\n    display(md(context))\n    display(md(\"### Answer: \"))\n    display(md(extract_content(generate_DS_answers(context))))\n    print('\\n')","metadata":{"execution":{"iopub.status.busy":"2024-04-13T21:27:40.49087Z","iopub.execute_input":"2024-04-13T21:27:40.491238Z","iopub.status.idle":"2024-04-13T21:27:59.970168Z","shell.execute_reply.started":"2024-04-13T21:27:40.491204Z","shell.execute_reply":"2024-04-13T21:27:59.969205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After fine-tuning, the Gemma model demonstrates a notable refinement in its outputs, delivering heightened precision and accuracy tailored to the specific queries posed. Through this process, Gemma has evolved to provide responses that are finely attuned to the intricacies of the questions posed, facilitating a more effective and insightful interaction with the user.\nThis is of great importance, and through this notebook, Gemma can also learn about topics it was previously unfamiliar with.","metadata":{}},{"cell_type":"markdown","source":"## Conclusion\n\nIn this beginner-friendly notebook, we've outlined the process of fine-tuning the Gemma model, a Large Language Model (LLM), specifically for Basic Data Science Concept generation. Starting from data loading, we've demonstrated how to train the Gemma model effectively, even for those new to working with LLMs.\n\nAchieving the best performance with the Gemma model (or any LLM) generally requires training with more extensive datasets and over more epochs. Future enhancements could include integrating Retrieval-Augmented Generation (RAG) and Direct Preference Optimization (DPO) training techniques, offering a way to further improve the model by incorporating external knowledge bases for more precise and relevant responses.\n\nUltimately, this notebook is designed to make the Gemma model approachable for beginners, illustrating that straightforward steps can unlock the potential of LLMs for diverse domain-specific tasks. It encourages users to experiment with the Gemma model across various fields, broadening the scope of its application and enhancing its utility.\n\n#### **If you find this notebook useful, please consider upvoting it.**\n\n#### **This will help others find it and encourage us to write more code, which benefits everyone.**","metadata":{}}]}