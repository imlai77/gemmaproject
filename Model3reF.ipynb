{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":64148,"databundleVersionId":7669720,"sourceType":"competition"},{"sourceId":7045515,"sourceType":"datasetVersion","datasetId":4054191},{"sourceId":7923451,"sourceType":"datasetVersion","datasetId":4633221},{"sourceId":11371,"sourceType":"modelInstanceVersion","modelInstanceId":5171}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":2365.313933,"end_time":"2024-02-27T21:18:25.620906","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-27T20:39:00.306973","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importation\n","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.set_option('display.max_colwidth', None)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2024-04-30T01:34:48.676033Z","iopub.execute_input":"2024-04-30T01:34:48.676448Z","iopub.status.idle":"2024-04-30T01:34:49.691867Z","shell.execute_reply.started":"2024-04-30T01:34:48.676415Z","shell.execute_reply":"2024-04-30T01:34:49.690862Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/data-assistants-with-gemma/submission_categories.txt\n/kaggle/input/data-assistants-with-gemma/submission_instructions.txt\n/kaggle/input/gemma/keras/gemma_2b_en/2/config.json\n/kaggle/input/gemma/keras/gemma_2b_en/2/tokenizer.json\n/kaggle/input/gemma/keras/gemma_2b_en/2/metadata.json\n/kaggle/input/gemma/keras/gemma_2b_en/2/model.weights.h5\n/kaggle/input/gemma/keras/gemma_2b_en/2/assets/tokenizer/vocabulary.spm\n/kaggle/input/1000-data-science-concepts/data_science_concepts.csv\n/kaggle/input/understanding-contextual-questions-answers/train.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Installation of Keras\n","metadata":{}},{"cell_type":"code","source":"# Install Keras 3 last. \n!pip install -q -U keras-nlp\n!pip install -q -U keras>=3","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-04-30T01:34:54.169169Z","iopub.execute_input":"2024-04-30T01:34:54.169551Z","iopub.status.idle":"2024-04-30T01:35:26.914737Z","shell.execute_reply.started":"2024-04-30T01:34:54.169523Z","shell.execute_reply":"2024-04-30T01:35:26.913685Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import sklearn\nimport pandas as pd\nimport numpy as np\nfrom wordcloud import WordCloud, STOPWORDS\nimport keras\nimport keras_nlp\nfrom IPython.display import display, Markdown\nimport matplotlib.pyplot as plt\nfrom keras_nlp.models import GemmaCausalLM\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-04-30T01:35:31.423690Z","iopub.execute_input":"2024-04-30T01:35:31.424080Z","iopub.status.idle":"2024-04-30T01:35:45.237467Z","shell.execute_reply.started":"2024-04-30T01:35:31.424048Z","shell.execute_reply":"2024-04-30T01:35:45.236611Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-04-30 01:35:34.274992: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-30 01:35:34.275093: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-30 01:35:34.394591: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Importing OS\n","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"torch\" or \"tensorflow\".\n# Avoid memory fragmentation on JAX backend.\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\"","metadata":{"execution":{"iopub.status.busy":"2024-04-30T01:35:52.047841Z","iopub.execute_input":"2024-04-30T01:35:52.048943Z","iopub.status.idle":"2024-04-30T01:35:52.053601Z","shell.execute_reply.started":"2024-04-30T01:35:52.048902Z","shell.execute_reply":"2024-04-30T01:35:52.052615Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Loading Dataset\nDataset Link: https://www.kaggle.com/datasets/hserdaraltan/1000-data-science-concepts\n\nThis dataset covers more than 1000 common data science concepts.","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/1000-data-science-concepts/data_science_concepts.csv',nrows=200)\ndata.head() # First 5 rows of the dataset","metadata":{"papermill":{"duration":0.043817,"end_time":"2024-02-27T20:39:03.856006","exception":false,"start_time":"2024-02-27T20:39:03.812189","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-30T01:35:58.337528Z","iopub.execute_input":"2024-04-30T01:35:58.338497Z","iopub.status.idle":"2024-04-30T01:35:58.377648Z","shell.execute_reply.started":"2024-04-30T01:35:58.338458Z","shell.execute_reply":"2024-04-30T01:35:58.376562Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                                               Question  \\\n0            What is under-fitting and overfitting in machine learning?   \n1       Can you explain what a false positive and a false negative are?   \n2                                      Clarify the concept of Phase IV.   \n3    What is semi-supervised learning described in a short description?   \n4  Discuss the parallelization of training in gradient boosting models.   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Answer  \n0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Underfitting is when a model is too simple, and overfitting is when it's too complex, making it perform poorly on new data.  \n1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                A false positive incorrectly indicates a condition is present when it's not, while a false negative misses detecting a condition that is there.  \n2                                                                    Phase IV studies, also known as post-marketing surveillance, are conducted after a drug or medical product is made available to the general public. They aim to monitor the product's safety, efficacy, and long-term effects in a larger and more diverse population, providing valuable insights into real-world usage. Phase IV studies help regulators, healthcare providers, and patients make informed decisions about the product's continued use by assessing its risks and benefits over an extended period outside the controlled environment of clinical trials.  \n3                                                                                                     Semi-supervised learning integrates both labeled and unlabeled data during model training. By leveraging the abundance of unlabeled data alongside limited labeled data, it enhances model performance and generalization to new examples, offering scalability and efficiency in scenarios where acquiring labeled data is resource-intensive or impractical. This approach bridges the gap between supervised and unsupervised learning, unlocking the potential of vast unlabeled datasets for training robust machine learning models.  \n4  Parallelizing training of a gradient boosting model is indeed possible, leveraging the parallel processing capabilities of modern hardware, such as GPUs. Frameworks like XGBoost offer options like 'tree_method = 'gpu_hist'' to utilize GPUs for faster training. By distributing computation across multiple cores or devices simultaneously, parallelization accelerates the training process, significantly reducing training time and improving efficiency. This approach is particularly beneficial for large datasets and complex models, where traditional sequential training may be computationally intensive and time-consuming.  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Question</th>\n      <th>Answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>What is under-fitting and overfitting in machine learning?</td>\n      <td>Underfitting is when a model is too simple, and overfitting is when it's too complex, making it perform poorly on new data.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Can you explain what a false positive and a false negative are?</td>\n      <td>A false positive incorrectly indicates a condition is present when it's not, while a false negative misses detecting a condition that is there.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Clarify the concept of Phase IV.</td>\n      <td>Phase IV studies, also known as post-marketing surveillance, are conducted after a drug or medical product is made available to the general public. They aim to monitor the product's safety, efficacy, and long-term effects in a larger and more diverse population, providing valuable insights into real-world usage. Phase IV studies help regulators, healthcare providers, and patients make informed decisions about the product's continued use by assessing its risks and benefits over an extended period outside the controlled environment of clinical trials.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>What is semi-supervised learning described in a short description?</td>\n      <td>Semi-supervised learning integrates both labeled and unlabeled data during model training. By leveraging the abundance of unlabeled data alongside limited labeled data, it enhances model performance and generalization to new examples, offering scalability and efficiency in scenarios where acquiring labeled data is resource-intensive or impractical. This approach bridges the gap between supervised and unsupervised learning, unlocking the potential of vast unlabeled datasets for training robust machine learning models.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Discuss the parallelization of training in gradient boosting models.</td>\n      <td>Parallelizing training of a gradient boosting model is indeed possible, leveraging the parallel processing capabilities of modern hardware, such as GPUs. Frameworks like XGBoost offer options like 'tree_method = 'gpu_hist'' to utilize GPUs for faster training. By distributing computation across multiple cores or devices simultaneously, parallelization accelerates the training process, significantly reducing training time and improving efficiency. This approach is particularly beneficial for large datasets and complex models, where traditional sequential training may be computationally intensive and time-consuming.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data.tail() # Last 5 rows of the dataset","metadata":{"execution":{"iopub.status.busy":"2024-04-30T01:36:03.360958Z","iopub.execute_input":"2024-04-30T01:36:03.361701Z","iopub.status.idle":"2024-04-30T01:36:03.371483Z","shell.execute_reply.started":"2024-04-30T01:36:03.361662Z","shell.execute_reply":"2024-04-30T01:36:03.370290Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                                                  Question  \\\n195              Why are activation functions required in neural networks?   \n196                      Can you explain a bidirectional search algorithm?   \n197         Do gradient descent methods always converge to similar points?   \n198                                                     Describe word2vec.   \n199  What is the difference between a generative and discriminative model?   \n\n                                                                                                                                                                                                                                              Answer  \n195                                                                    Activation functions introduce nonlinearity, enabling neural networks to learn complex relationships between inputs and outputs, enhancing model capacity and expressiveness.  \n196  A bidirectional search algorithm runs two simultaneous searches: one forward from the starting point and one backward from the goal. The aim is to meet in the middle, thus potentially finding a solution faster than a unidirectional search.  \n197                                                                                                    Gradient descent methods may converge to different local optima, which depend on the starting conditions and the nature of the cost function.  \n198                                                                                                                     Word2vec is a suite of models used to produce word embeddings, trained to predict surrounding words in a linguistic context.  \n199                                                          Generative models learn data categories, while discriminative models learn category distinctions. Discriminative models generally outperform generative models in classification tasks.  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Question</th>\n      <th>Answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>195</th>\n      <td>Why are activation functions required in neural networks?</td>\n      <td>Activation functions introduce nonlinearity, enabling neural networks to learn complex relationships between inputs and outputs, enhancing model capacity and expressiveness.</td>\n    </tr>\n    <tr>\n      <th>196</th>\n      <td>Can you explain a bidirectional search algorithm?</td>\n      <td>A bidirectional search algorithm runs two simultaneous searches: one forward from the starting point and one backward from the goal. The aim is to meet in the middle, thus potentially finding a solution faster than a unidirectional search.</td>\n    </tr>\n    <tr>\n      <th>197</th>\n      <td>Do gradient descent methods always converge to similar points?</td>\n      <td>Gradient descent methods may converge to different local optima, which depend on the starting conditions and the nature of the cost function.</td>\n    </tr>\n    <tr>\n      <th>198</th>\n      <td>Describe word2vec.</td>\n      <td>Word2vec is a suite of models used to produce word embeddings, trained to predict surrounding words in a linguistic context.</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>What is the difference between a generative and discriminative model?</td>\n      <td>Generative models learn data categories, while discriminative models learn category distinctions. Discriminative models generally outperform generative models in classification tasks.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Gemma Model\n**Gemma Model** is a collection of lightweight open-source generative AI (GenAI) models developed by Google DeepMind. \n\n**Inputs and Outputs**\n* Input: Gemma models take in text strings, which can range from questions and prompts to longer documents that require summarization.\n* Output: In response, they generate text in English, offering answers, summaries, or other forms of text-based output, tailored to the input provided.\n","metadata":{"papermill":{"duration":0.007713,"end_time":"2024-02-27T20:39:44.261984","exception":false,"start_time":"2024-02-27T20:39:44.254271","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"This code snippet creates an instance of the GemmaCausalLM model and assigns it to the variable gemma_lm. It creates the model from a preset configuration named \"gemma_instruct_2b_en\".This preset specifies the architecture, hyperparameters, and other settings for the model.","metadata":{}},{"cell_type":"code","source":"#Create the model using the from_preset method\ngemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")\ngemma_lm.summary()","metadata":{"_kg_hide-output":true,"papermill":{"duration":55.420002,"end_time":"2024-02-27T20:40:39.689056","exception":false,"start_time":"2024-02-27T20:39:44.269054","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-30T01:36:08.887889Z","iopub.execute_input":"2024-04-30T01:36:08.888681Z","iopub.status.idle":"2024-04-30T01:37:39.754116Z","shell.execute_reply.started":"2024-04-30T01:36:08.888648Z","shell.execute_reply":"2024-04-30T01:37:39.753248Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'task.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'model.weights.h5' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'preprocessor.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nnormalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,506,172,416\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"## Test Model\n","metadata":{}},{"cell_type":"code","source":"from memory_profiler import memory_usage\n\n# Define input text\ninput_text = \"What is supervised machine learning?\"\n\n# Function to measure memory usage\ndef measure_memory(func):\n    mem_usage = memory_usage((func, (input_text,), {}))\n    return max(mem_usage)  # Maximum memory usage during function execution\n\n# Define the function to be executed\ndef generate_with_memory(input_text):\n    outputs = gemma_lm.generate(input_text, max_length=64)\n    return outputs\n\ngemma_lm.generate(\"What is supervised machine learning?\", max_length=64)\n","metadata":{"papermill":{"duration":12.003086,"end_time":"2024-02-27T20:40:51.699777","exception":false,"start_time":"2024-02-27T20:40:39.696691","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-30T01:40:28.902551Z","iopub.execute_input":"2024-04-30T01:40:28.903194Z","iopub.status.idle":"2024-04-30T01:40:30.275188Z","shell.execute_reply.started":"2024-04-30T01:40:28.903162Z","shell.execute_reply":"2024-04-30T01:40:30.274218Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'What is supervised machine learning?\\n\\nSupervised machine learning is a type of machine learning where the algorithm is trained on a set of labeled data. The algorithm is then used to predict the outcome of new data.\\n\\nSupervised machine learning is a type of machine learning where the algorithm is trained on a set of labeled'"},"metadata":{}}]}]}