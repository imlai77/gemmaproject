{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":64148,"databundleVersionId":7669720,"sourceType":"competition"},{"sourceId":7923451,"sourceType":"datasetVersion","datasetId":4633221},{"sourceId":28785,"sourceType":"modelInstanceVersion","modelInstanceId":8318}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Importation**","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-28T20:44:01.328620Z","iopub.execute_input":"2024-04-28T20:44:01.329616Z","iopub.status.idle":"2024-04-28T20:45:33.963582Z","shell.execute_reply.started":"2024-04-28T20:44:01.329581Z","shell.execute_reply":"2024-04-28T20:45:33.962486Z"}}},{"cell_type":"code","source":"!pip install -q -U huggingface_hub\n!pip install -q -U transformers\n!pip install -q -U accelerate\n!pip install -q -U BitsAndBytes\n%pip install -q trl\n%pip install -q peft","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Addition of torch, os, transfromers and other relavant parameters","metadata":{}},{"cell_type":"code","source":"import torch\nimport os\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nfrom transformers import AutoTokenizer, AutoModelForCausalLM,BitsAndBytesConfig, AutoConfig, TrainingArguments, pipeline\nfrom wordcloud import WordCloud, STOPWORDS\nfrom datasets import Dataset\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nfrom trl import SFTTrainer\nfrom IPython.display import Markdown as md\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-28T20:45:54.735822Z","iopub.execute_input":"2024-04-28T20:45:54.736469Z","iopub.status.idle":"2024-04-28T20:46:13.509416Z","shell.execute_reply.started":"2024-04-28T20:45:54.736428Z","shell.execute_reply":"2024-04-28T20:46:13.508615Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-04-28 20:46:03.836628: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-28 20:46:03.836729: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-28 20:46:03.956971: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Loading Gemma Model\n\n* `BitsAndBytesConfig`\n\n\n* `Loading Tokenizer and Model with Quantization`","metadata":{}},{"cell_type":"code","source":"%%time\ntokenizer= AutoTokenizer.from_pretrained(\"/kaggle/input/gemma/transformers/2b-it/3\")\nquantization_config=BitsAndBytesConfig(\n                    load_in_4bit=True,\n                    bnb_4bit_use_double_quant=True,\n                    bnb_4bit_quant_type='nf4',\n                    bnb_4bit_compute_dtype=torch.bfloat16,)\nmodel = AutoModelForCausalLM.from_pretrained(\"/kaggle/input/gemma/transformers/2b-it/3\",quantization_config=quantization_config,low_cpu_mem_usage=True)\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T20:46:30.164984Z","iopub.execute_input":"2024-04-28T20:46:30.165604Z","iopub.status.idle":"2024-04-28T20:47:11.418616Z","shell.execute_reply.started":"2024-04-28T20:46:30.165568Z","shell.execute_reply":"2024-04-28T20:47:11.417615Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Gemma's activation function should be approximate GeLU and not exact GeLU.\nChanging the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0282fa79800493f872bd0a7bc6a62ba"}},"metadata":{}},{"name":"stdout","text":"GemmaForCausalLM(\n  (model): GemmaModel(\n    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n    (layers): ModuleList(\n      (0-17): 18 x GemmaDecoderLayer(\n        (self_attn): GemmaSdpaAttention(\n          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n          (up_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n          (down_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n          (act_fn): PytorchGELUTanh()\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n    )\n    (norm): GemmaRMSNorm()\n  )\n  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n)\nCPU times: user 7.85 s, sys: 4.88 s, total: 12.7 s\nWall time: 41.2 s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 4. Q & A Using Gemma\n\n* The code measures the execution time of generating a text summary using a pre-trained Gemma model. It initializes an input text, tokenizes it, and generates a summary using the model. \n* The generated summary is then decoded and printed. \n* This process is timed using the `%%time` magic command.The execution time of the entire process is displayed. \n* The Gemma model utilizes the GPU for faster computation. \n* The summary length is limited to 256 tokens.","metadata":{}},{"cell_type":"code","source":"#---------------------------------------------------------------------------\n\nimport time\nimport torch\n\ninput_text = 'Answer common questions about the Python programming language.'\ninput_ids = tokenizer(input_text, return_tensors='pt').to(\"cuda\")\n\n# Function to get memory usage on CUDA device\ndef get_cuda_memory_usage():\n    return torch.cuda.memory_allocated() / 1024 / 1024  # Memory usage in MB\n\n# Memory usage before execution\nmemory_before = get_cuda_memory_usage()\n\n# Execution\noutputs = model.generate(**input_ids, max_new_tokens=256)\n\n# Memory usage after execution\nmemory_after = get_cuda_memory_usage()\n\nmemory_used = memory_after - memory_before\n\nprint(tokenizer.decode(outputs[0]))\nprint('')\nprint(\"Memory used:\", memory_used, \"MB\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T20:48:14.494936Z","iopub.execute_input":"2024-04-28T20:48:14.495616Z","iopub.status.idle":"2024-04-28T20:48:28.184602Z","shell.execute_reply.started":"2024-04-28T20:48:14.495582Z","shell.execute_reply":"2024-04-28T20:48:28.183673Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"<bos>Answer common questions about the Python programming language.\n\n**1. What is Python?**\n\n* Python is a high-level, interpreted programming language.\n* It is known for its clear and concise syntax, making it easier to learn and use than other programming languages.\n* Python is widely used for various purposes, including data science, machine learning, web development, and scripting.\n\n**2. What are the key features of Python?**\n\n* **Dynamic typing:** Python does not require you to explicitly declare the data type of variables.\n* **Indentation:** Python uses indentation to define blocks of code, making it clear and readable.\n* **Modules:** Python has a vast collection of modules that extend the functionality of the language.\n* **Concurrency:** Python supports multithreading, allowing multiple tasks to run concurrently.\n* **Regular expressions:** Python provides powerful regular expression capabilities for text manipulation.\n\n**3. What are the different types of data in Python?**\n\n* **Numbers:** Python supports various numerical data types, including integers and floating-point numbers.\n* **Strings:** Strings are sequences of characters enclosed in double quotes.\n* **Booleans:** True and False are used to represent boolean values.\n* **Lists:** Lists are ordered collections of items of the same\n\nMemory used: 8.13623046875 MB\n","output_type":"stream"}]}]}